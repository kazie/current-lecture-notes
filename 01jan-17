Generic structure of parallel computers
	A perallel computer is a collection of concurrently executing, communicating processing elements
	Potential for largeperformance boost:
		  1000 processors run 1000 times faster than one processor
	Or, tackle a problem 1000 time the size, accuracy, ...
	A parallel computer increases memory capacity &c

	A collection of P processor--memory paries that communicates with eachother

	When there is a global memory we have a shared memory architecture, also known as a multiprocessor
	Where there is no global memory we have a "share nothing" architecture (also known as a distributed
	memory architecture), also known as a multicomputer.

	Ferlin is both multiprocessor and multicomputer, because it is a cluster of multicored computers.
	Each processor as a cache and each computer has a shared memory, the among is cache is a trade off.
	For each computer the memory is treated as with shared memory only.



A first example: rank sort:

  	Problem: Given n (pairwise distinct) numbers. Sort the numbers in increasing order.
	
	
	Solution:
	
	Let the rank r_i of an element a_i of the set M of numbers be the number
	of elements being less then that element,

	   r_i = #{a_j :in: M | a_j < a_i}
	
	In a fully sorted list, the position of element i is just it rank r_i.

	   C:
		for (i = 0; i q n; i++)
		    {
			r[i] = 0;
			for (j = 0; j < n; j++)
			    if (a[j] < a[i])
			         r[i]++;
		    }

	   FORTRAN-version available in slides.


	The innermost loops (and the loop initialisation) are completely
	independent of each other.
	(more on slide)

	
	
T*_s donates the execution time of the fastest serial algorithm.

The parallel reuntime T_P processors is the time which elapses from the moment
a parallel execution starts to the moment the last processing element finishes execution.

The parallel speedup S_P is the qoutient of T_P and T*_s.

        S_P = T*_s / T_P.

The best possible speedup is S_P = P.

By definition, T*_s <= T_1, the execution time of a prarallel algorithm on one processor.

Equality does not hold in general.  (!)

Fortunately, for many algorithms we have T*_s :~=: T_1.

Estimating the efficiency of a parallel program includes also an estimation of memory and
communication overhead.



The execution time on a one processor, for rank sort, is T_1 = :O:(n^2).

Assume that we have a shared memory machine (multpiprocessor). In that case, there is no
additional memory needed and no communuication overhead. The body of the i-loop has
executio time t_i (1 + n + r_i)R_a = :O:(n), where r_a is the time for one arithmetic operation.

The parallel execution time is, for n <= P. T_P = max_{0 <= i < n} t_i = 2n.



Fact
	The best known sequential sorting algorithm has complexity T*_s = :O:(n log n).
	
Corollary
	Rank sort on a shared memory machine hs parallel speed up of S_P = T*_s  / T_P = :O:(log P)
	// TODO rest fo slide //


Will use MATLAB-like syntax for describing algorithms
MATLAB is not an environment for parallel programming!
All array adressing is 0-based
Processes are counted 0-based



Amdahl's Law (1967)

	 Assume that, for a given algorithm, there is a certain fraction f of the
	 computation which cannot be divided into concurrent tasks.

	 The serial part is then t_s = fT_1

	 On P processing element, we have T_P = fT__1 + (1 - f)T_1 / P

	 The speedup becomes S_p = T_1/(fT_1 + (1 - f)T_1/P) = P/(1 + (P - 1)f)

	 Conclusion

		// TODO see slide //

	AmdahlÃ¤s law led many to take a pessimistic outlook onto the benefits on parallelism.

	Question
		So what is wrong with this reasoining?

	Observation
		Amdahl's law assumes that the workload W remains fixed.

	Fact
		But parallel computers are used t otackle more ambitious workloads
		- W increases with P
		- f often decreases with W

	Gustafson's Law (1988)

	Assume that the parallel execution time T_p is constant (Note: For Amdahl's law, the
	sequential execution time is fixed)

	Let f' the fraction of serial time spent in the parallel program

	Nor it holds T_1 = f'T_P + (1 - f')PT_P

	Scaled speedup	   S'_P = f' + (1 - f')P

	The scaled speedup can become arbitrarily large.

