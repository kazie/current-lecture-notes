Generic structure of parallel computers
	A perallel computer is a collection of concurrently executing, communicating processing elements
	Potential for largeperformance boost:
		  1000 processors run 1000 times faster than one processor
	Or, tackle a problem 1000 time the size, accuracy, ...
	A parallel computer increases memory capacity &c

	A collection of P processor--memory paries that communicates with eachother

	When there is a global memory we have a shared memory architecture, also known as a multiprocessor
	Where there is no global memory we have a "share nothing" architecture (also known as a distributed
	memory architecture), also known as a multicomputer.

	Ferlin is both multiprocessor and multicomputer, because it is a cluster of multicored computers.
	Each processor as a cache and each computer has a shared memory, the among is cache is a trade off.
	For each computer the memory is treated as with shared memory only.



A first example: rank sort:

  	Problem: Given n (pairwise distinct) numbers. Sort the numbers in increasing order.
	
	
	Solution:
	
	Let the rank r_i of an element a_i of the set M of numbers be the number
	of elements being less then that element,

	   r_i = #{a_j :in: M | a_j < a_i}
	
	In a fully sorted list, the position of element i is just it rank r_i.

	   C:
		for (i = 0; i q n; i++)
		    {
			r[i] = 0;
			for (j = 0; j < n; j++)
			    if (a[j] < a[i])
			         r[i]++;
		    }

	   FORTRAN-version available in slides.


	The innermost loops (and the loop initialisation) are completely
	independent of each other.
	(more on slide)

	
	
T*_s donates the execution time of the fastest serial algorithm.

The parallel reuntime T_P processors is the time which elapses from the moment
a parallel execution starts to the moment the last processing element finishes execution.

The parallel speedup S_P is the qoutient of T_P and T*_s.

        S_P = T*_s / T_P.

The best possible speedup is S_P = P.

By definition, T*_s <= T_1, the execution time of a prarallel algorithm on one processor.

Equality does not hold in general.  (!)

Fortunately, for many algorithms we have T*_s :~=: T_1.

Estimating the efficiency of a parallel program includes also an estimation of memory and
communication overhead.



The execution time on a one processor, for rank sort, is T_1 = :O:(n^2).

Assume that we have a shared memory machine (multpiprocessor). In that case, there is no
additional memory needed and no communuication overhead. The body of the i-loop has
executio time t_i (1 + n + r_i)R_a = :O:(n), where r_a is the time for one arithmetic operation.

The parallel execution time is, for n <= P. T_P = max_{0 <= i < n} t_i = 2n.

